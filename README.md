# Simple Neural Network Without Backpropagation

This project implements basic neural networks from scratch that learn through random weight adjustments rather than traditional backpropagation. The networks attempt to learn a simple addition operation.

## Overview

The project contains two implementations:

### 1. Basic Implementation (1_neural_net_without_backprop.py)

- 2 neurons arranged in 2 layers
- First neuron takes 2 inputs (a=5, b=9)
- Second neuron takes the output from first neuron
- Goal is to learn the addition operation (a + b = 14)
- Uses ReLU activation function
- Random weight adjustment learning strategy
- Real-time training visualization in terminal

### 2. Advanced Implementation (2_neural_net_train_and_test.py)

- Same neural network architecture as basic version
- Added features:
  - Weight saving/loading functionality
  - Interactive testing interface
  - User-friendly terminal UI with menu system
  - Ability to test custom input values
  - Progress tracking with reduced output clutter

## Features

- Built from scratch using only NumPy
- No backpropagation - uses random weight adjustments
- Colored terminal output showing training progress
- Error tracking and best weights saving
- Comprehensive error handling
- Multiple implementations for learning purposes

## â¤ï¸ Support & Get 400+ AI Projects

This is one of 400+ fascinating projects in my collection! [Support me on Patreon](https://www.patreon.com/c/echohive42/membership) to get:

- ğŸ¯ Access to 400+ AI projects (and growing daily!)
  - Including advanced projects like [2 Agent Real-time voice template with turn taking](https://www.patreon.com/posts/2-agent-real-you-118330397)
- ğŸ“¥ Full source code & detailed explanations
- ğŸ“š 1000x Cursor Course
- ğŸ“ Live coding sessions & AMAs
- ğŸ’¬ 1-on-1 consultations (higher tiers)
- ğŸ Exclusive discounts on AI tools & platforms (up to $180 value)
