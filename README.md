# Simple Neural Network Without Backpropagation

This project implements a basic neural network from scratch that learns through random weight adjustments rather than traditional backpropagation. The network attempts to learn a simple addition operation.

## Overview

The neural network consists of:
- 2 neurons arranged in 2 layers
- First neuron takes 2 inputs (a=5, b=9)
- Second neuron takes the output from first neuron
- Goal is to learn the addition operation (a + b = 14)
- Uses ReLU activation function
- Random weight adjustment learning strategy

## Features

- Built from scratch using only NumPy
- No backpropagation - uses random weight adjustments
- Colored terminal output showing training progress
- Error tracking and best weights saving
- Comprehensive error handling

- ## â¤ï¸ Support & Get 400+ AI Projects

This is one of 400+ fascinating projects in my collection! [Support me on Patreon](https://www.patreon.com/c/echohive42/membership) to get:

- ğŸ¯ Access to 400+ AI projects (and growing daily!)
  - Including advanced projects like [2 Agent Real-time voice template with turn taking](https://www.patreon.com/posts/2-agent-real-you-118330397)
- ğŸ“¥ Full source code & detailed explanations
- ğŸ“š 1000x Cursor Course
- ğŸ“ Live coding sessions & AMAs
- ğŸ’¬ 1-on-1 consultations (higher tiers)
- ğŸ Exclusive discounts on AI tools & platforms (up to $180 value)


## Requirements 
